{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86cab7bc-d128-4759-8857-33d2b480b620",
   "metadata": {},
   "source": [
    "# Library imports, configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb0e13c-f8d9-44cc-ad54-b29afc2ca97c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import cv2\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, randint\n",
    "from PIL import Image\n",
    "import pyvips\n",
    "import torch\n",
    "from joblib import dump\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from cloudpathlib import S3Path, S3Client, CloudPath\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score, mean_squared_error, precision_recall_curve, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import binarize, LabelEncoder, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "# from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout\n",
    "# from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# import tensorflow_decision_forests as tfdf\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e586f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "nom du GPU : NVIDIA A2\n",
      "GPU initialisé :  True\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() : device= torch.device(\"cuda:0\" ) \n",
    "else : device  = \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "if device != \"cpu\" :\n",
    "    print(\"nom du GPU :\", torch.cuda.get_device_name(device=None))\n",
    "    print(\"GPU initialisé : \", torch.cuda.is_initialized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ccf2d5d-84cb-4d73-aea8-495415f508fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1965e11-6da1-4989-af77-332ac61692f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e929bc-b32d-409a-a1f9-0d71c235f446",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jplaton/diffusion/.keep',\n",
       " 'jplaton/diffusion/ted',\n",
       " 'jplaton/diffusion/visio_mel']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUCKET = \"jplaton/diffusion\"\n",
    "fs.ls(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d8435-f3fb-44f2-9da5-a92db084e066",
   "metadata": {},
   "source": [
    "# DATA Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa4da4-e041-47d9-9498-ae9877d00565",
   "metadata": {},
   "source": [
    "## Metadata and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ece4bd5-0ed8-4fb0-bc1b-7cd8b0ff2c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATH_TRAIN = \"visio_mel/train_labels.csv\"\n",
    "FILE_PATH_TRAIN_S3 = BUCKET + \"/\" + FILE_PATH_TRAIN\n",
    "\n",
    "with fs.open(FILE_PATH_TRAIN_S3, mode=\"rb\") as file_in:\n",
    "    train_labels = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a665d2-1646-40e5-ac69-2e7369877ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATH_TRAIN = \"visio_mel/train_metadata.csv\"\n",
    "FILE_PATH_TRAIN_S3 = BUCKET + \"/\" + FILE_PATH_TRAIN\n",
    "\n",
    "with fs.open(FILE_PATH_TRAIN_S3, mode=\"rb\") as file_in:\n",
    "    train_metadata = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58ef29cb-1379-4f8f-8bb1-2423b5650780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATH_TEST = \"visio_mel/visiomel_code_execution_development_data/test_labels.csv\"\n",
    "FILE_PATH_TEST_S3 = BUCKET + \"/\" + FILE_PATH_TEST\n",
    "\n",
    "with fs.open(FILE_PATH_TEST_S3, mode=\"rb\") as file_in:\n",
    "    test_labels = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1975147-9b12-4a6e-92f9-84e82f4737a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATH_TEST = \"visio_mel/visiomel_code_execution_development_data/test_metadata.csv\"\n",
    "FILE_PATH_TEST_S3 = BUCKET + \"/\" + FILE_PATH_TEST\n",
    "\n",
    "with fs.open(FILE_PATH_TEST_S3, mode=\"rb\") as file_in:\n",
    "    test_metadata = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af98f87-a877-420e-b461-a2607032e930",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a8c3eca-88de-4f2f-88a3-91bb2baa3b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select a set of images to plot\n",
    "SEED = 42\n",
    "NUM_IMAGES = 3\n",
    "\n",
    "# we'll use the US url\n",
    "selected_image_paths = train_metadata.sample(\n",
    "    random_state=SEED, n=NUM_IMAGES\n",
    ").us_tif_url.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209e935-8680-4ddb-a5c2-7882b3158214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_page(page_num, image_paths):\n",
    "    fig, axes = plt.subplots(1, len(image_paths), figsize=(10, 10))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for n, image_s3_path in enumerate(image_paths):\n",
    "        fname = S3Path(image_s3_path).name\n",
    "        print(f\"Downloading {fname}\")\n",
    "        local_file = S3Path(image_s3_path,client = S3Client(no_sign_request=True)).fspath #local_cache_dir='/tmp/images', \n",
    "\n",
    "        n_frames = Image.open(local_file).n_frames\n",
    "        img = pyvips.Image.new_from_file(local_file, page=page_num).numpy()\n",
    "\n",
    "        axes[n].set_title(f\"{fname}, page={page_num}\\n {img.shape}\\n\")\n",
    "        axes[n].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b5570-accb-4ac8-8da6-c6f81c7a04cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's look at a few high-resolution pages from our selected images\n",
    "visualize_page(3, selected_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25074666-fd8a-4f30-a36b-4b9086f3c023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's look at low-resolution pages from the same images\n",
    "visualize_page(8, selected_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e393ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = pyvips.Image.new_from_file('../data/images/zyo5fbtd.tif', page=8).numpy()\n",
    "img = cv2.resize(img, (250, 250))\n",
    "plt.imshow(np.array(img))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0546122",
   "metadata": {},
   "source": [
    "### Création du dataset d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_file = os.listdir(\"../data/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff6703",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_im = train_labels.copy()\n",
    "train_labels_im = train_labels_im[train_labels_im['filename'].isin(train_images_file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31761806",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_im = train_metadata.copy()\n",
    "train_metadata_im = train_metadata_im[train_metadata_im['filename'].isin(train_images_file)]\n",
    "train_metadata_im['filepaths'] = '../data/images/'+ train_metadata_im['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_paths = np.array(train_metadata_im['filepaths'])\n",
    "train_images_labels = np.array(train_labels_im['relapse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images_name = np.array(train_metadata['filename'])\n",
    "# train_images_labels_all = np.array(train_labels['relapse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la classe permettant d'accéder au dataset\n",
    "# On crée ici la classe Custom Dataset héritant de la classe Dataset en python\n",
    "# Un CustomDataset sera initialisé avec l'ensemble des images_paths et ses labels précedemment présentés.\n",
    "# Un objet Dataset sera dit itérable dans la mesure où on pourra boucler dessus pour en récupérer les éléments. A chaque itération d'une telle boucle La fonction getitem  sera appelée pour accéder à un élément. \n",
    "# Ci-dessous on demande à chaque itération de retourner un dictionnnaire dont les items (clef,valeur)  sont : \n",
    "# ('image', l'image sous forme d'array ) ('label', le label associé (0,1))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths,labels): \n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        #local_file = S3Path('s3://drivendata-competition-visiomel-public-us/images/'+self.image_paths[idx],client=S3Client(local_cache_dir='/tmp/images',no_sign_request=True)).fspath \n",
    "        #image = pyvips.Image.new_from_file(local_file, page=3).numpy()\n",
    "        image = pyvips.Image.new_from_file(self.image_paths[idx], page=0).numpy()  \n",
    "        #image = cv2.resize(image, (256, 256))\n",
    "        image = torch.tensor(np.array(image,dtype = float)/255, dtype =torch.float).permute(2,1,0)     \n",
    "        label = self.labels[idx]\n",
    "        #os.remove(local_file)\n",
    "     \n",
    "        return {\"image\": image, \"label\" : label} \n",
    "        \n",
    "        \n",
    "    def __len__(self):  # return count of sample we have\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57b3426a",
   "metadata": {},
   "source": [
    "###  Définition de quelques hyper paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c086ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASSES = 2\n",
    "config ={\n",
    "    'n_epoch' : 100,\n",
    "    'val_size' : 0.20,\n",
    "    'batch_size' : 4,\n",
    "    'optimizer' : \"SGD\",\n",
    "    'lr' : 0.005,\n",
    "    'momentum' : 0.9,\n",
    "    'model type': \"convnet\",\n",
    "    'descriptif': \"Entrainement avec un convnet\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3792d8a7",
   "metadata": {},
   "source": [
    "### Création des Data Loader\n",
    "Dans un premier temps on va découper notre jeu d'apprentissage (les 366 images labellisées) en : \n",
    "un jeu de d'entraînement sur lequel on  entraînera notre modèle un jeu de validation ne participant pas à l'entrainement du modèle mais permettant d'en évaluer les performances\n",
    "La fonction train_test_split de scikit learn permet de faire ce découpage. Le paramètre val_size de la config donne la proportion ddu jeu d'images total (entre 0 et 1) que l'on veut garder dans le jeu de validation. Le paramètre stratify permet de préciser sur quel critère on veut stratifier cette sélection aléatoire (ici les labels dans le but d'avoir un échantillon représentatif des labels de l'ensemble des images)Lors de l'entraînement, on va en fait entrainer notre algorithme par itération sur des petits paquets d'images (appelés batchs). On calcule pour chacun de ces batchs l'erreur commise par l'algorithme et on modifie ses paramètres en conséquence (par descente de gradient)Cette extraction par paquet d'images est permise par le Dataloader moyenant le remplissage du paramètre batch_size (cf 'batch_size' dans la config) permettant de préciser la taille des batchs que l'on souhaite avoir. Le paramètre shuffle égal à True veut dire que les images seront toutes remélangées une fois un tour total de l'ensemble des images réalisé. \n",
    "On appelle un tour complet une epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1bfd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  config['batch_size']\n",
    "all_dataset = CustomDataset(train_images_paths,train_images_labels)\n",
    "#all_dataset = CustomDataset(train_images_name,train_images_labels_all)\n",
    "\n",
    "# je découpe le data set en train et validation (20 % de validation) en stratifiant par les labels\n",
    "train_indices, valid_indices = train_test_split(list(range(len(train_images_labels))), test_size=config['val_size'], stratify=train_images_labels)\n",
    "#train_indices, valid_indices = train_test_split(list(range(len(train_images_labels_all))), test_size=config['val_size'], stratify=train_images_labels_all)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(all_dataset, train_indices)\n",
    "valid_dataset = torch.utils.data.Subset(all_dataset, valid_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f4d88be",
   "metadata": {},
   "source": [
    "### Définition des modèles\n",
    " On définit ici une classe Net (héritant de la classe module) permettant d'instancier des réseaux de neurones convolutifs.Dans init, on définit l'ensemble des objets utilisés dans l'opération forward.L'opération forward définit la suite des opérations qui seront appliquées aux images auxquelles on appliquera un modèle de type Net.Dans la fonction forward (qui prend en entrée une image x) on constate bien que plusieurs transformations enboitées sont successivement appliquées à l'image x en entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96531d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 4)\n",
    "        self.conv4 = nn.Conv2d(32, 16, 3)\n",
    "        self.conv5 = nn.Conv2d(16, 24, 3)\n",
    "        self.fc1 = nn.Linear(600, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, NB_CLASSES)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = torch.flatten(x, 1)# flatten all dimensions except batch\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf5a0cb9",
   "metadata": {},
   "source": [
    "###  Test Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822a924",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "get_n_params(net) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59578b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=config['lr'])\n",
    "net = net.to(device)\n",
    "\n",
    "entropy = nn.CrossEntropyLoss() \n",
    "\n",
    "liste_loss = []\n",
    "liste_acc_val = []\n",
    "for epoch in range(config['n_epoch']):  \n",
    "    \n",
    "    net = net.to(device)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    t= tqdm(train_loader, desc=\"epoch %i\" % (epoch+1),position = 0, leave=True)\n",
    "    epoch_loop = enumerate(t)\n",
    "\n",
    "    for i, data in epoch_loop:\n",
    "        \n",
    "        taille_batch = data['image'].shape[0]\n",
    "        images = data['image']\n",
    "        labels  =  data['label']\n",
    "            \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        pred = net(images)\n",
    "            \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = entropy(pred,labels)\n",
    "            \n",
    "        loss.backward() # calculer le gradient\n",
    "        optimizer.step() # avancer dans le sens du gradient calculé\n",
    "\n",
    "        del images, labels, pred\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (i+1) % 10 == 0:  \n",
    "            # ici enregistrement de la loss sur le train, sur le validation et envoi des résultats à wnandb\n",
    "            liste_loss.append(running_loss)\n",
    "\n",
    "            # validation\n",
    "            ech_val = iter(valid_loader)\n",
    "            dico = next(ech_val)\n",
    "            images =dico[\"image\"]\n",
    "            labels =dico[\"label\"]\n",
    "\n",
    "            pred = torch.argmax(net(images.to(device)),dim =1)\n",
    "            labels = labels.to(device)\n",
    "                \n",
    "            acc_val =round(100*int(torch.sum(pred == labels).cpu())/int(np.array(labels.size())),2)\n",
    "            liste_acc_val.append(acc_val)\n",
    "            t.set_description(\"epoch %i, 'mean loss: %.6f','acc.val: %.2f'\" % (epoch+1,running_loss/10,acc_val))\n",
    "            t.refresh()\n",
    "                \n",
    "            running_loss =0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbacd196",
   "metadata": {},
   "source": [
    "###  Evaluation du réseau entraîné\n",
    " Lors de l'entraînement, les moyennes de la loss sur le jeu d'entrâinement et de la précision obtenue sur le jeu de validation sont enregistrées tous les 5 batchs dans :\n",
    "liste_loss liste_acc_val\n",
    "On peut donc les représenter graphiquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('batchs')\n",
    "ax1.set_ylabel('valiadation accuracy', color=color)\n",
    "ax1.plot(liste_acc_val, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('entropy loss', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(liste_loss, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3124f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = iter(valid_loader)\n",
    "dico = next(t)\n",
    "\n",
    "images =dico[\"image\"]\n",
    "labels =dico[\"label\"]\n",
    "\n",
    "pred = np.array(torch.argmax(net(images.to(device)),dim =1).cpu())\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(pred) \n",
    "print(labels)\n",
    "\n",
    "confusion_matrix(pred,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96327793-de9d-4b89-bafd-f025d8efea19",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8fb870-1255-42da-9363-4d1a14a32f17",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "To start, let's do some feature engineering! We'll first take a look at the unique values for each variable in the metadatatrain_metadata.age.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8fc1c82-98e0-4537-ae99-2ee1ca884f06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metadata.sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "378fdb86-e820-4948-81fb-4f0a8790b7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['thigh', 'trunc', 'face', 'forearm', 'arm', 'leg', 'hand', nan,\n",
       "       'foot', 'sole', 'finger', 'neck', 'toe', 'seat', 'scalp', 'nail',\n",
       "       'trunk', 'lower limb/hip', 'hand/foot/nail', 'head/neck',\n",
       "       'upper limb/shoulder'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metadata.body_site.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ce91f3a-6ff4-4ebe-a542-fdb2ad6d9f48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['YES', 'NO', nan], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metadata.melanoma_history.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4710dfc5-4c20-417b-8311-a52912da4b7a",
   "metadata": {},
   "source": [
    "So far, we've learned:\n",
    "\n",
    "- age is available to us in two-year intervals\n",
    "- melanoma_history is sometimes nan\n",
    "- body_site categories are not all at the same level of aggregation - for example, some slides are labeled as being from \"hand/foot/nail\" while some slides are labeled as from the hand, foot, and nail individually.\n",
    "\n",
    "Let's do the following:\n",
    "\n",
    "- convert age to integer values\n",
    "- convert melanoma_history to dummy variables\n",
    "- For this first pass, we will not use body_site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3688449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['melanoma_history_YES', 'melanoma_history_nan', 'body_site_face',\n",
       "       'body_site_finger', 'body_site_foot', 'body_site_forearm',\n",
       "       'body_site_hand', 'body_site_hand_foot_nail',\n",
       "       'body_site_head_neck', 'body_site_leg', 'body_site_lower_limb_hip',\n",
       "       'body_site_nail', 'body_site_neck', 'body_site_scalp',\n",
       "       'body_site_seat', 'body_site_sole', 'body_site_thigh',\n",
       "       'body_site_toe', 'body_site_trunc', 'body_site_trunk',\n",
       "       'body_site_upper_limb_shoulder', 'body_site_nan'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define an encoder for melanoma_history, body_site \n",
    "enc = OneHotEncoder(drop=\"first\", sparse_output=False)\n",
    "df_enc = train_metadata[['melanoma_history','body_site']].copy()\n",
    "df_enc['body_site']=train_metadata['body_site'].str.replace(' ','_').str.replace('/','_')\n",
    "enc.fit(df_enc)\n",
    "enc.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87c4c9e1-293f-4c1c-ad03-a81e8595b45e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_feats(df=train_metadata):\n",
    "    feats = df.copy()\n",
    "    # take the first age in the range and convert to integer\n",
    "    feats[\"age_int\"] = feats.age.str.slice(1, 3).astype(int)\n",
    "    feats['body_site']=feats['body_site'].str.replace(' ','_').str.replace('/','_')\n",
    "    X = pd.concat(\n",
    "        [\n",
    "            feats[[\"age_int\", \"sex\",\"resolution\"]],\n",
    "            pd.DataFrame(\n",
    "                enc.transform(feats[[\"melanoma_history\",\"body_site\"]]),\n",
    "                columns=enc.get_feature_names_out(),\n",
    "                index=feats.index,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ba04c6a-9967-4c34-9f21-01b3b211d876",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_int                            int64\n",
       "sex                                int64\n",
       "resolution                       float64\n",
       "melanoma_history_YES             float64\n",
       "melanoma_history_nan             float64\n",
       "body_site_face                   float64\n",
       "body_site_finger                 float64\n",
       "body_site_foot                   float64\n",
       "body_site_forearm                float64\n",
       "body_site_hand                   float64\n",
       "body_site_hand_foot_nail         float64\n",
       "body_site_head_neck              float64\n",
       "body_site_leg                    float64\n",
       "body_site_lower_limb_hip         float64\n",
       "body_site_nail                   float64\n",
       "body_site_neck                   float64\n",
       "body_site_scalp                  float64\n",
       "body_site_seat                   float64\n",
       "body_site_sole                   float64\n",
       "body_site_thigh                  float64\n",
       "body_site_toe                    float64\n",
       "body_site_trunc                  float64\n",
       "body_site_trunk                  float64\n",
       "body_site_upper_limb_shoulder    float64\n",
       "body_site_nan                    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess features\n",
    "\n",
    "X = preprocess_feats(train_metadata)\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e408f266-f41f-460b-a145-61e1c80d6417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take labels from train_labels.csv\n",
    "\n",
    "y = train_labels.relapse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c309ee4-3acc-480d-9e7e-98a6ea453f99",
   "metadata": {},
   "source": [
    "## Model training and calibration\n",
    "For a first pass at training, we will have a simple stratified 80-20 split between train and test. The stratified split helps ensure that the proportion of relapse cases are similar between our train and test set and that we are performing our model training on a representative subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b7b6cc2-059a-4c1b-8805-7720ef59afef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f38a2cdc-1ba3-4fb9-842a-c0e06c07e7fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_int</th>\n",
       "      <th>sex</th>\n",
       "      <th>resolution</th>\n",
       "      <th>melanoma_history_YES</th>\n",
       "      <th>melanoma_history_nan</th>\n",
       "      <th>body_site_face</th>\n",
       "      <th>body_site_finger</th>\n",
       "      <th>body_site_foot</th>\n",
       "      <th>body_site_forearm</th>\n",
       "      <th>body_site_hand</th>\n",
       "      <th>...</th>\n",
       "      <th>body_site_neck</th>\n",
       "      <th>body_site_scalp</th>\n",
       "      <th>body_site_seat</th>\n",
       "      <th>body_site_sole</th>\n",
       "      <th>body_site_thigh</th>\n",
       "      <th>body_site_toe</th>\n",
       "      <th>body_site_trunc</th>\n",
       "      <th>body_site_trunk</th>\n",
       "      <th>body_site_upper_limb_shoulder</th>\n",
       "      <th>body_site_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>0.220512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>0.226490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>0.242797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>0.264384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.263223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0.220512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>269 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age_int  sex  resolution  melanoma_history_YES  melanoma_history_nan   \n",
       "1242       52    2    0.220512                   0.0                   0.0  \\\n",
       "460        26    2    0.226490                   1.0                   0.0   \n",
       "614        56    2    0.242797                   0.0                   0.0   \n",
       "317        70    2    0.264384                   0.0                   0.0   \n",
       "263        80    1    0.250000                   0.0                   1.0   \n",
       "...       ...  ...         ...                   ...                   ...   \n",
       "353        50    2    0.250000                   0.0                   0.0   \n",
       "174        46    1    0.250000                   0.0                   1.0   \n",
       "1045       66    1    0.263223                   0.0                   0.0   \n",
       "905        42    2    0.250000                   0.0                   0.0   \n",
       "973        40    2    0.220512                   0.0                   0.0   \n",
       "\n",
       "      body_site_face  body_site_finger  body_site_foot  body_site_forearm   \n",
       "1242             0.0               0.0             0.0                0.0  \\\n",
       "460              0.0               0.0             0.0                0.0   \n",
       "614              0.0               0.0             0.0                0.0   \n",
       "317              0.0               0.0             0.0                0.0   \n",
       "263              0.0               0.0             0.0                0.0   \n",
       "...              ...               ...             ...                ...   \n",
       "353              0.0               0.0             0.0                0.0   \n",
       "174              0.0               0.0             0.0                0.0   \n",
       "1045             0.0               0.0             0.0                0.0   \n",
       "905              1.0               0.0             0.0                0.0   \n",
       "973              0.0               0.0             0.0                0.0   \n",
       "\n",
       "      body_site_hand  ...  body_site_neck  body_site_scalp  body_site_seat   \n",
       "1242             0.0  ...             0.0              0.0             0.0  \\\n",
       "460              0.0  ...             0.0              0.0             0.0   \n",
       "614              0.0  ...             0.0              0.0             0.0   \n",
       "317              0.0  ...             0.0              0.0             0.0   \n",
       "263              0.0  ...             0.0              0.0             0.0   \n",
       "...              ...  ...             ...              ...             ...   \n",
       "353              0.0  ...             0.0              0.0             0.0   \n",
       "174              0.0  ...             0.0              0.0             0.0   \n",
       "1045             0.0  ...             0.0              0.0             0.0   \n",
       "905              0.0  ...             0.0              0.0             0.0   \n",
       "973              0.0  ...             0.0              0.0             0.0   \n",
       "\n",
       "      body_site_sole  body_site_thigh  body_site_toe  body_site_trunc   \n",
       "1242             0.0              0.0            0.0              0.0  \\\n",
       "460              0.0              0.0            0.0              0.0   \n",
       "614              0.0              0.0            0.0              1.0   \n",
       "317              0.0              0.0            0.0              1.0   \n",
       "263              0.0              0.0            0.0              1.0   \n",
       "...              ...              ...            ...              ...   \n",
       "353              0.0              1.0            0.0              0.0   \n",
       "174              0.0              0.0            0.0              1.0   \n",
       "1045             0.0              0.0            0.0              0.0   \n",
       "905              0.0              0.0            0.0              0.0   \n",
       "973              0.0              0.0            0.0              0.0   \n",
       "\n",
       "      body_site_trunk  body_site_upper_limb_shoulder  body_site_nan  \n",
       "1242              0.0                            0.0            0.0  \n",
       "460               0.0                            0.0            1.0  \n",
       "614               0.0                            0.0            0.0  \n",
       "317               0.0                            0.0            0.0  \n",
       "263               0.0                            0.0            0.0  \n",
       "...               ...                            ...            ...  \n",
       "353               0.0                            0.0            0.0  \n",
       "174               0.0                            0.0            0.0  \n",
       "1045              0.0                            0.0            0.0  \n",
       "905               0.0                            0.0            0.0  \n",
       "973               0.0                            0.0            0.0  \n",
       "\n",
       "[269 rows x 25 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc46f6b3-fb18-4a35-905d-1fd93131faad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relapse\n",
       "0    0.841566\n",
       "1    0.158434\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25827b95-5413-48ee-9e30-0ddbe9c3bdf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relapse\n",
       "0    0.840149\n",
       "1    0.159851\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_best_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3379b-1244-4071-99d6-21fac713e3ae",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e55109",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "params = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "search = RandomizedSearchCV(rf, param_distributions=params, random_state=42, n_iter=200, cv=5, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "search.fit(X_train,y_train)\n",
    "\n",
    "report_best_scores(search.cv_results_,n_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a1d9d-4825-4ce1-8f10-a539e9c102ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit a calibrated random forest model\n",
    "\n",
    "rf = RandomForestClassifier(random_state=SEED,n_estimators = 200,min_samples_split = 2,min_samples_leaf = 4,\n",
    "max_features = 'sqrt', max_depth = 2, bootstrap = True)\n",
    "calibrated_rf = CalibratedClassifierCV(rf, method=\"sigmoid\", cv=5)\n",
    "calibrated_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3af29b-d3eb-4313-a3a9-a54a224bc90d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "353e149c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m modelGB \u001b[39m=\u001b[39m GradientBoostingClassifier()\n\u001b[1;32m      3\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m: uniform(),\n\u001b[1;32m      4\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msubsample\u001b[39m\u001b[39m\"\u001b[39m    : uniform(),\n\u001b[1;32m      5\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m\"\u001b[39m : randint(\u001b[39m100\u001b[39m, \u001b[39m500\u001b[39m),\n\u001b[1;32m      6\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m\"\u001b[39m    : randint(\u001b[39m2\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[1;32m      7\u001b[0m                  }\n\u001b[0;32m----> 9\u001b[0m search \u001b[39m=\u001b[39m RandomizedSearchCV(xgb_model, param_distributions\u001b[39m=\u001b[39mparams, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, n_iter\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, return_train_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m search\u001b[39m.\u001b[39mfit(X_train,y_train)\n\u001b[1;32m     13\u001b[0m report_best_scores(search\u001b[39m.\u001b[39mcv_results_,n_top\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb_model' is not defined"
     ]
    }
   ],
   "source": [
    "modelGB = GradientBoostingClassifier()\n",
    "\n",
    "params = {\"learning_rate\": uniform(),\n",
    "            \"subsample\"    : uniform(),\n",
    "                  \"n_estimators\" : randint(100, 500),\n",
    "                  \"max_depth\"    : randint(2, 10)\n",
    "                 }\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=5, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "search.fit(X_train,y_train)\n",
    "\n",
    "report_best_scores(search.cv_results_,n_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033ea30-9a55-4009-9a86-0762a98d1b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelGB = GradientBoostingClassifier(learning_rate = 0.01083765148029836,n_estimators=175,\n",
    "max_depth = 2, subsample = 0.3549051904627206)\n",
    "calibrated_GB = CalibratedClassifierCV(modelGB, method=\"sigmoid\", cv=5)\n",
    "modelGB_fit = calibrated_GB.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "582006ba",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd990817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m\"\u001b[39m: randint(\u001b[39m2\u001b[39m, \u001b[39m10\u001b[39m), \u001b[39m# default 3\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m\"\u001b[39m: randint(\u001b[39m100\u001b[39m, \u001b[39m200\u001b[39m), \u001b[39m# default 100\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msubsample\u001b[39m\u001b[39m\"\u001b[39m: uniform(\u001b[39m0.6\u001b[39m, \u001b[39m0.4\u001b[39m)\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     11\u001b[0m search \u001b[39m=\u001b[39m RandomizedSearchCV(xgb_model, param_distributions\u001b[39m=\u001b[39mparams, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, n_iter\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, return_train_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m search\u001b[39m.\u001b[39;49mfit(X_train,y_train)\n\u001b[1;32m     15\u001b[0m report_best_scores(search\u001b[39m.\u001b[39mcv_results_,n_top\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1769\u001b[0m         ParameterSampler(\n\u001b[1;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[1;32m   1771\u001b[0m         )\n\u001b[1;32m   1772\u001b[0m     )\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/xgboost/sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1462\u001b[0m (\n\u001b[1;32m   1463\u001b[0m     model,\n\u001b[1;32m   1464\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1470\u001b[0m )\n\u001b[1;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[1;32m   1488\u001b[0m )\n\u001b[0;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1491\u001b[0m     params,\n\u001b[1;32m   1492\u001b[0m     train_dmatrix,\n\u001b[1;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1502\u001b[0m )\n\u001b[1;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": randint(2, 10), # default 3\n",
    "    \"n_estimators\": randint(100, 200), # default 100\n",
    "    \"learning_rate\": uniform(0.01, 0.3), # default 0.1 \n",
    "    \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "    \"subsample\": uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=5, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "search.fit(X_train,y_train)\n",
    "\n",
    "report_best_scores(search.cv_results_,n_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "random_state=SEED,\n",
    "max_depth = 2,\n",
    "n_estimators = 193,\n",
    "learning_rate = 0.023800792606525824,\n",
    "colsample_bytree=0.7580363202534581,\n",
    "subsample=0.9421842336044028)\n",
    "calibrated_xgb = CalibratedClassifierCV(xgb_model, method=\"sigmoid\", cv=5)\n",
    "xbg_model_fit = calibrated_xgb.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a8b1793",
   "metadata": {},
   "source": [
    "### Neural Network (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc273171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input functions\n",
    "\n",
    "batch_size = 100\n",
    "train_steps = 1000\n",
    "\n",
    "\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    return dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features=dict(features)\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17fb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Tensorflow feature columns\n",
    "feature_columns = [tf.feature_column.numeric_column(x) for x in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an Estimator\n",
    "\n",
    "# Build a DNN with 2 hidden layers and 10 nodes in each hidden layer.\n",
    "model = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                    hidden_units=[1024, 512, 256],\n",
    "                                    optimizer=tf.keras.optimizers.legacy.Adam(lr=0.0001))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75eeef20",
   "metadata": {},
   "source": [
    "### Train, Evaluate, and Predict\n",
    "Now that we have an Estimator object, we can call methods to do the following:\n",
    "\n",
    "Train the model.\n",
    "Evaluate the trained model.\n",
    "Use the trained model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc68f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "#The steps argument tells the method to stop training after a number of training steps.\n",
    "\n",
    "model.train(input_fn=lambda:train_input_fn(X_train, y_train, batch_size), steps=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the trained model\n",
    "#Now that the model has been trained, we can get some statistics on its performance. The following code block evaluates the accuracy of the trained model on the test data.\n",
    "# Evaluate the model.\n",
    "eval_result = model.evaluate(\n",
    "    input_fn=lambda:eval_input_fn(X_test, y_test, batch_size))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.2f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f64b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9dc47-c668-4506-9b82-34887acdbeac",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78902390-08ec-433e-b0ba-8e7f19009f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lr = LogisticRegression()\n",
    "# stack = StackingClassifier(classifiers=[calibrated_rf, calibrated_GB], meta_classifier=lr)\n",
    "# stack.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4a357-d710-42ff-856b-8c57f8598bfc",
   "metadata": {},
   "source": [
    "## Predicting on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5465ed3-8d86-4543-90ee-f1474f7aa285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score(y_true, y_pred):\n",
    "    return log_loss(y_true, y_pred, eps=1e-16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94b045-be34-421f-a5b6-847f064e14d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = calibrated_rf.predict_proba(X_test)[:, 1]\n",
    "score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b03ad-8795-47c8-97e3-b965beac11eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_GB = modelGB_fit.predict_proba(X_test)[:, 1]\n",
    "score(y_test, preds_GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_xgb = xbg_model_fit.predict_proba(X_test)[:, 1]\n",
    "score(y_test, preds_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b0d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(model.predict(\n",
    "    input_fn=lambda:eval_input_fn(X_test, y_test, batch_size)))\n",
    "\n",
    "# Dictionary for predictions\n",
    "col1 = []\n",
    "col2 = []\n",
    "col3 = []\n",
    "\n",
    "\n",
    "for idx, input, p in zip(X_test.index, y_test, predictions):\n",
    "    v  = p[\"class_ids\"][0] \n",
    "    class_id = p['class_ids'][0]\n",
    "    probability = p['probabilities'][1] # Probability\n",
    "    \n",
    "    # Adding to dataframe\n",
    "    col1.append(idx) # Index\n",
    "    col2.append(probability) # Prediction\n",
    "    col3.append(input) # Expecter\n",
    "    \n",
    "   \n",
    "    #print(template.format(idx, v, 100 * probability, input))\n",
    "\n",
    "\n",
    "results = pd.DataFrame({'index':col1, 'prediction_proba':col2, 'expected':col3})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e506ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score(results.expected,results.prediction_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f8011-ee16-41c2-982c-b481cacfb66e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preds_stack = stack.predict_proba(X_test)[:, 1]\n",
    "# score(y_test, preds_stack)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ede8d155",
   "metadata": {},
   "source": [
    "### TEST SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e23544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_feats(df, enc):\n",
    "    feats = df.copy()\n",
    "    # take the first age in the range and convert to integer\n",
    "    feats[\"age_int\"] = feats.age.str.slice(1, 3).astype(int)\n",
    "    feats['body_site']=feats['body_site'].str.replace(' ','_').str.replace('/','_')\n",
    "    X = pd.concat(\n",
    "        [\n",
    "            feats[[\"age_int\", \"sex\",\"resolution\"]],\n",
    "            pd.DataFrame(\n",
    "                enc.transform(feats[[\"melanoma_history\",\"body_site\"]]),\n",
    "                columns=enc.get_feature_names_out(),\n",
    "                index=feats.index,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return X\n",
    "\n",
    "processed_features = preprocess_feats(test_metadata, enc)\n",
    "\n",
    "predsub = calibrated_xgb.predict_proba(processed_features)[:,1]\n",
    "\n",
    "score(test_labels.relapse,predsub)\n",
    "\n",
    "# predictions = list(model.predict(\n",
    "#     input_fn=lambda:eval_input_fn(processed_features, test_labels.relapse, batch_size)))\n",
    "\n",
    "# # Dictionary for predictions\n",
    "# col1 = []\n",
    "# col2 = []\n",
    "# col3 = []\n",
    "\n",
    "\n",
    "# for idx, input, p in zip(test_metadata.index, test_labels, predictions):\n",
    "#     v  = p[\"class_ids\"][0] \n",
    "#     class_id = p['class_ids'][0]\n",
    "#     probability = p['probabilities'][1] # Probability\n",
    "    \n",
    "#     # Adding to dataframe\n",
    "#     col1.append(idx) # Index\n",
    "#     col2.append(probability) # Prediction\n",
    "#     col3.append(input) # Expecter\n",
    "    \n",
    "   \n",
    "#     #print(template.format(idx, v, 100 * probability, input))\n",
    "\n",
    "\n",
    "# results = pd.DataFrame({'index':col1, 'prediction_proba':col2, 'expected':col3})\n",
    "# results.head()\n",
    "\n",
    "# score(results.expected,results.prediction_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f4364b-7502-4479-bfc9-87661c3dfa65",
   "metadata": {},
   "source": [
    "# CODE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ea4aa-f01a-45e4-84e2-768964f24445",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"submission/assets\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# save out model to assets directory in submission folder\n",
    "dump(calibrated_rf, \"submission/assets/random_forest_model.joblib\")\n",
    "dump(calibrated_GB, \"submission/assets/gradient_boosting_model.joblib\")\n",
    "dump(xbg_model_fit, \"submission/assets/xgboost_model.joblib\")\n",
    "\n",
    "# save out encoder to assets directory as well\n",
    "dump(enc, \"submission/assets/hotencoder.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0d952-016b-456d-97f1-fe973634618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "\n",
    "\"\"\"Solution for VisioMel Challenge\"\"\"\n",
    "\n",
    "from joblib import load\n",
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DATA_ROOT = Path(\"/code_execution/data/\")\n",
    "\n",
    "\n",
    "def preprocess_feats(df, enc):\n",
    "    feats = df.copy()\n",
    "    # take the first age in the range and convert to integer\n",
    "    feats[\"age_int\"] = feats.age.str.slice(1, 3).astype(int)\n",
    "    feats['body_site']=feats['body_site'].str.replace(' ','_').str.replace('/','_')\n",
    "    X = pd.concat(\n",
    "        [\n",
    "            feats[[\"age_int\", \"sex\",\"resolution\"]],\n",
    "            pd.DataFrame(\n",
    "                enc.transform(feats[[\"melanoma_history\",\"body_site\"]]),\n",
    "                columns=enc.get_feature_names_out(),\n",
    "                index=feats.index,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return X\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    # load sumission format\n",
    "    submission_format = pd.read_csv(DATA_ROOT / \"submission_format.csv\", index_col=0)\n",
    "    \n",
    "    # load test_metadata\n",
    "    test_metadata = pd.read_csv(DATA_ROOT / \"test_metadata.csv\", index_col=0)\n",
    "    \n",
    "    logger.info(\"Loading feature encoder and model\")\n",
    "    calibrated_xgb = load(\"assets/xgboost_model.joblib\")\n",
    "    enc = load(\"assets/hotencoder.joblib\")\n",
    "    \n",
    "    logger.info(\"Preprocessing features\")\n",
    "    processed_features = preprocess_feats(test_metadata, enc)\n",
    "        \n",
    "    logger.info(\"Checking test feature filenames are in the same order as the submission format\")\n",
    "    assert (processed_features.index == submission_format.index).all()\n",
    "    \n",
    "    logger.info(\"Checking test feature columns align with loaded model\")\n",
    "    assert (processed_features.columns == calibrated_xgb.feature_names_in_).all()\n",
    "    \n",
    "    logger.info(\"Generating predictions\")\n",
    "    submission_format[\"relapse\"] = calibrated_xgb.predict_proba(processed_features)[:,1]\n",
    "\n",
    "    # save as \"submission.csv\" in the root folder, where it is expected\n",
    "    logger.info(\"Writing out submission.csv\")\n",
    "    submission_format.to_csv(\"submission.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe005f-2879-4a67-92ba-f2575d47a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip up submission\n",
    "! cd submission; zip -r ../submission.zip ./*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
