{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86cab7bc-d128-4759-8857-33d2b480b620",
   "metadata": {},
   "source": [
    "# Library imports, configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0e13c-f8d9-44cc-ad54-b29afc2ca97c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import cv2\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, randint\n",
    "from PIL import Image\n",
    "import pyvips\n",
    "import torch\n",
    "from joblib import dump\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from cloudpathlib import S3Path, S3Client, CloudPath\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score, mean_squared_error, precision_recall_curve, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import binarize, LabelEncoder, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "# from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout\n",
    "# from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# import tensorflow_decision_forests as tfdf\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e586f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() : device= torch.device(\"cuda:0\" ) \n",
    "else : device  = \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "if device != \"cpu\" :\n",
    "    print(\"nom du GPU :\", torch.cuda.get_device_name(device=None))\n",
    "    print(\"GPU initialisé : \", torch.cuda.is_initialized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf2d5d-84cb-4d73-aea8-495415f508fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1965e11-6da1-4989-af77-332ac61692f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e929bc-b32d-409a-a1f9-0d71c235f446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET = \"jplaton/diffusion\"\n",
    "fs.ls(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d8435-f3fb-44f2-9da5-a92db084e066",
   "metadata": {},
   "source": [
    "# DATA Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa4da4-e041-47d9-9498-ae9877d00565",
   "metadata": {},
   "source": [
    "## Metadata and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece4bd5-0ed8-4fb0-bc1b-7cd8b0ff2c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATH_TRAIN = \"visio_mel/train_labels.csv\"\n",
    "FILE_PATH_TRAIN_S3 = BUCKET + \"/\" + FILE_PATH_TRAIN\n",
    "\n",
    "with fs.open(FILE_PATH_TRAIN_S3, mode=\"rb\") as file_in:\n",
    "    train_labels = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a665d2-1646-40e5-ac69-2e7369877ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATH_TRAIN = \"visio_mel/train_metadata.csv\"\n",
    "FILE_PATH_TRAIN_S3 = BUCKET + \"/\" + FILE_PATH_TRAIN\n",
    "\n",
    "with fs.open(FILE_PATH_TRAIN_S3, mode=\"rb\") as file_in:\n",
    "    train_metadata = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef29cb-1379-4f8f-8bb1-2423b5650780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATH_TEST = \"visio_mel/visiomel_code_execution_development_data/test_labels.csv\"\n",
    "FILE_PATH_TEST_S3 = BUCKET + \"/\" + FILE_PATH_TEST\n",
    "\n",
    "with fs.open(FILE_PATH_TEST_S3, mode=\"rb\") as file_in:\n",
    "    test_labels = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1975147-9b12-4a6e-92f9-84e82f4737a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATH_TEST = \"visio_mel/visiomel_code_execution_development_data/test_metadata.csv\"\n",
    "FILE_PATH_TEST_S3 = BUCKET + \"/\" + FILE_PATH_TEST\n",
    "\n",
    "with fs.open(FILE_PATH_TEST_S3, mode=\"rb\") as file_in:\n",
    "    test_metadata = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af98f87-a877-420e-b461-a2607032e930",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c3eca-88de-4f2f-88a3-91bb2baa3b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select a set of images to plot\n",
    "SEED = 42\n",
    "NUM_IMAGES = 3\n",
    "\n",
    "# we'll use the US url\n",
    "selected_image_paths = train_metadata.sample(\n",
    "    random_state=SEED, n=NUM_IMAGES\n",
    ").us_tif_url.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209e935-8680-4ddb-a5c2-7882b3158214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_page(page_num, image_paths):\n",
    "    fig, axes = plt.subplots(1, len(image_paths), figsize=(10, 10))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for n, image_s3_path in enumerate(image_paths):\n",
    "        fname = S3Path(image_s3_path).name\n",
    "        print(f\"Downloading {fname}\")\n",
    "        local_file = S3Path(image_s3_path,client = S3Client(no_sign_request=True)).fspath #local_cache_dir='/tmp/images', \n",
    "\n",
    "        n_frames = Image.open(local_file).n_frames\n",
    "        img = pyvips.Image.new_from_file(local_file, page=page_num).numpy()\n",
    "\n",
    "        axes[n].set_title(f\"{fname}, page={page_num}\\n {img.shape}\\n\")\n",
    "        axes[n].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b5570-accb-4ac8-8da6-c6f81c7a04cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's look at a few high-resolution pages from our selected images\n",
    "visualize_page(3, selected_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25074666-fd8a-4f30-a36b-4b9086f3c023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's look at low-resolution pages from the same images\n",
    "visualize_page(8, selected_image_paths)\n",
    "img = pyvips.Image.new_from_file('../data/images/9k10x5h3.tif', page=0).numpy()\n",
    "#plt.imshow(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f91ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray = rgb2gray(img)\n",
    "img_tresh = img_gray < 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c86795",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_tresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff76e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray[img_tresh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bis = pyvips.Image.new_from_file('../data/images/9k10x5h3.tif', page=8)\n",
    "#print(img_bis.width, img_bis.height)\n",
    "img_bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7964c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_bis)\n",
    "plt.plot(53696, 103232, \"og\", markersize=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f152c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0546122",
   "metadata": {},
   "source": [
    "### Création du dataset d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_file = os.listdir(\"../data/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff6703",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_im = train_labels.copy()\n",
    "train_labels_im = train_labels_im[train_labels_im['filename'].isin(train_images_file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31761806",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_im = train_metadata.copy()\n",
    "train_metadata_im = train_metadata_im[train_metadata_im['filename'].isin(train_images_file)]\n",
    "train_metadata_im['filepaths'] = '../data/images/'+ train_metadata_im['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_paths = np.array(train_metadata_im['filepaths'])\n",
    "train_images_labels = np.array(train_labels_im['relapse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a441d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images_name = np.array(train_metadata['filename'])\n",
    "# train_images_labels_all = np.array(train_labels['relapse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e00f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_df = X.copy()\n",
    "tabular_df = tabular_df[tabular_df['filename'].isin(train_images_file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la classe permettant d'accéder au dataset\n",
    "# On crée ici la classe Custom Dataset héritant de la classe Dataset en python\n",
    "# Un CustomDataset sera initialisé avec l'ensemble des images_paths et ses labels précedemment présentés.\n",
    "# Un objet Dataset sera dit itérable dans la mesure où on pourra boucler dessus pour en récupérer les éléments. A chaque itération d'une telle boucle La fonction getitem  sera appelée pour accéder à un élément. \n",
    "# Ci-dessous on demande à chaque itération de retourner un dictionnnaire dont les items (clef,valeur)  sont : \n",
    "# ('image', l'image sous forme d'array ) ('label', le label associé (0,1))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths,labels,tabular_df): \n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.tabular = tabular_df\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        tabular = self.tabular.iloc[idx, 0:]\n",
    "        \n",
    "        #local_file = S3Path('s3://drivendata-competition-visiomel-public-us/images/'+self.image_paths[idx],client=S3Client(local_cache_dir='/tmp/images',no_sign_request=True)).fspath \n",
    "        #image = pyvips.Image.new_from_file(local_file, page=3).numpy()\n",
    "        image = pyvips.Image.new_from_file(self.image_paths[idx], page=3).numpy()  \n",
    "        image = cv2.resize(image, (512, 512))\n",
    "        image = torch.tensor(np.array(image,dtype = float)/255, dtype =torch.float).permute(2,1,0)     \n",
    "        label = self.labels[idx]\n",
    "        #os.remove(local_file)\n",
    "\n",
    "        tabular = tabular[['age_int','sex','resolution','melanoma_history_YES', 'melanoma_history_nan', 'body_site_face',\n",
    "       'body_site_finger', 'body_site_foot', 'body_site_forearm',\n",
    "       'body_site_hand', 'body_site_hand_foot_nail',\n",
    "       'body_site_head_neck', 'body_site_leg', 'body_site_lower_limb_hip',\n",
    "       'body_site_nail', 'body_site_neck', 'body_site_scalp',\n",
    "       'body_site_seat', 'body_site_sole', 'body_site_thigh',\n",
    "       'body_site_toe', 'body_site_trunc', 'body_site_trunk',\n",
    "       'body_site_upper_limb_shoulder', 'body_site_nan']]\n",
    "\n",
    "        tabular = tabular.tolist()\n",
    "        tabular = torch.FloatTensor(tabular)\n",
    "        \n",
    "     \n",
    "        return {\"image\": image, \"tabular\": tabular, \"label\" : label} \n",
    "        \n",
    "        \n",
    "    def __len__(self):  # return count of sample we have\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57b3426a",
   "metadata": {},
   "source": [
    "###  Définition de quelques hyper paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c086ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASSES = 2\n",
    "config ={\n",
    "    'n_epoch' : 100,\n",
    "    'val_size' : 0.20,\n",
    "    'batch_size' : 4,\n",
    "    'optimizer' : \"SGD\",\n",
    "    'lr' : 0.005,\n",
    "    'momentum' : 0.9,\n",
    "    'model type': \"convnet\",\n",
    "    'descriptif': \"Entrainement avec un convnet\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3792d8a7",
   "metadata": {},
   "source": [
    "### Création des Data Loader\n",
    "Dans un premier temps on va découper notre jeu d'apprentissage (les 366 images labellisées) en : \n",
    "un jeu de d'entraînement sur lequel on  entraînera notre modèle un jeu de validation ne participant pas à l'entrainement du modèle mais permettant d'en évaluer les performances\n",
    "La fonction train_test_split de scikit learn permet de faire ce découpage. Le paramètre val_size de la config donne la proportion ddu jeu d'images total (entre 0 et 1) que l'on veut garder dans le jeu de validation. Le paramètre stratify permet de préciser sur quel critère on veut stratifier cette sélection aléatoire (ici les labels dans le but d'avoir un échantillon représentatif des labels de l'ensemble des images)Lors de l'entraînement, on va en fait entrainer notre algorithme par itération sur des petits paquets d'images (appelés batchs). On calcule pour chacun de ces batchs l'erreur commise par l'algorithme et on modifie ses paramètres en conséquence (par descente de gradient)Cette extraction par paquet d'images est permise par le Dataloader moyenant le remplissage du paramètre batch_size (cf 'batch_size' dans la config) permettant de préciser la taille des batchs que l'on souhaite avoir. Le paramètre shuffle égal à True veut dire que les images seront toutes remélangées une fois un tour total de l'ensemble des images réalisé. \n",
    "On appelle un tour complet une epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1bfd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  config['batch_size']\n",
    "all_dataset = CustomDataset(train_images_paths,train_images_labels,tabular_df)\n",
    "#all_dataset = CustomDataset(train_images_name,train_images_labels_all)\n",
    "\n",
    "# je découpe le data set en train et validation (20 % de validation) en stratifiant par les labels\n",
    "train_indices, valid_indices = train_test_split(list(range(len(train_images_labels))), test_size=config['val_size'], stratify=train_images_labels)\n",
    "#train_indices, valid_indices = train_test_split(list(range(len(train_images_labels_all))), test_size=config['val_size'], stratify=train_images_labels_all)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(all_dataset, train_indices)\n",
    "valid_dataset = torch.utils.data.Subset(all_dataset, valid_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f4d88be",
   "metadata": {},
   "source": [
    "### Définition des modèles\n",
    " On définit ici une classe Net (héritant de la classe module) permettant d'instancier des réseaux de neurones convolutifs.Dans init, on définit l'ensemble des objets utilisés dans l'opération forward.L'opération forward définit la suite des opérations qui seront appliquées aux images auxquelles on appliquera un modèle de type Net.Dans la fonction forward (qui prend en entrée une image x) on constate bien que plusieurs transformations enboitées sont successivement appliquées à l'image x en entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_size, output_size):\n",
    "    block = nn.Sequential(\n",
    "        nn.Conv2d(input_size, output_size, (3, 3)), nn.ReLU(), nn.BatchNorm2d(output_size), nn.MaxPool2d((2, 2)),\n",
    "    )\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96531d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_block(3, 16)\n",
    "        self.conv2 = conv_block(16, 32)\n",
    "        self.conv3 = conv_block(32, 64)\n",
    "\n",
    "        self.ln1 = nn.Linear(246016, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(16)\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.ln2 = nn.Linear(16, 5)\n",
    "\n",
    "        self.ln4 = nn.Linear(25, 10)\n",
    "        self.ln5 = nn.Linear(10, 10)\n",
    "        self.ln6 = nn.Linear(10, 5)\n",
    "        self.ln7 = nn.Linear(10, NB_CLASSES)\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # self.conv3 = nn.Conv2d(16, 32, 4)\n",
    "        # self.conv4 = nn.Conv2d(32, 16, 3)\n",
    "        # self.conv5 = nn.Conv2d(16, 24, 3)\n",
    "        # self.fc1 = nn.Linear(256, 128)\n",
    "        # self.fc2 = nn.Linear(128, 64)\n",
    "        # self.fc3 = nn.Linear(64, NB_CLASSES)\n",
    "        \n",
    "    def forward(self, img, tab):\n",
    "        \n",
    "        img = self.conv1(img)\n",
    "        img = self.conv2(img)\n",
    "        img = self.conv3(img)\n",
    "        img = img.reshape(img.shape[0], -1)\n",
    "        img = self.ln1(img)\n",
    "        img = self.relu(img)\n",
    "        img = self.batchnorm(img)\n",
    "        img = self.dropout(img)\n",
    "        img = self.ln2(img)\n",
    "        img = self.relu(img)\n",
    "\n",
    "        tab = self.ln4(tab)\n",
    "        tab = self.relu(tab)\n",
    "        tab = self.ln5(tab)\n",
    "        tab = self.relu(tab)\n",
    "        tab = self.ln6(tab)\n",
    "        tab = self.relu(tab)\n",
    "\n",
    "        x = torch.cat((img, tab), dim=1)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return self.ln7(x)\n",
    "        # x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x = self.pool(F.relu(self.conv3(x)))\n",
    "        # x = self.pool(F.relu(self.conv4(x)))\n",
    "        # x = self.pool(F.relu(self.conv5(x)))\n",
    "        # x = torch.flatten(x, 1)# flatten all dimensions except batch\n",
    "        # #print(x.size())\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        # return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf5a0cb9",
   "metadata": {},
   "source": [
    "###  Test Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822a924",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "get_n_params(net) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59578b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=config['lr'])\n",
    "net = net.to(device)\n",
    "\n",
    "entropy = nn.CrossEntropyLoss() \n",
    "\n",
    "liste_loss = []\n",
    "liste_acc_val = []\n",
    "for epoch in range(config['n_epoch']):  \n",
    "    \n",
    "    net = net.to(device)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    t= tqdm(train_loader, desc=\"epoch %i\" % (epoch+1),position = 0, leave=True)\n",
    "    epoch_loop = enumerate(t)\n",
    "\n",
    "    for i, data in epoch_loop:\n",
    "        \n",
    "        taille_batch = data['image'].shape[0]\n",
    "        images = data['image']\n",
    "        labels  =  data['label']\n",
    "        tabular = data['tabular']\n",
    "            \n",
    "        images, labels, tabular = images.to(device), labels.to(device), tabular.to(device)\n",
    "\n",
    "        pred = net(images,tabular)\n",
    "            \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = entropy(pred,labels)\n",
    "            \n",
    "        loss.backward() # calculer le gradient\n",
    "        optimizer.step() # avancer dans le sens du gradient calculé\n",
    "\n",
    "        del images, labels, pred\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (i+1) % 10 == 0:  \n",
    "            # ici enregistrement de la loss sur le train, sur le validation et envoi des résultats à wnandb\n",
    "            liste_loss.append(running_loss)\n",
    "\n",
    "            # validation\n",
    "            ech_val = iter(valid_loader)\n",
    "            dico = next(ech_val)\n",
    "            images =dico[\"image\"]\n",
    "            labels =dico[\"label\"]\n",
    "            tabular =dico[\"tabular\"]\n",
    "\n",
    "            pred = torch.argmax(net(images.to(device),tabular.to(device)),dim =1)\n",
    "            labels = labels.to(device)\n",
    "                \n",
    "            acc_val =round(100*int(torch.sum(pred == labels).cpu())/int(np.array(labels.size())),2)\n",
    "            liste_acc_val.append(acc_val)\n",
    "            t.set_description(\"epoch %i, 'mean loss: %.6f','acc.val: %.2f'\" % (epoch+1,running_loss/10,acc_val))\n",
    "            t.refresh()\n",
    "                \n",
    "            running_loss =0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbacd196",
   "metadata": {},
   "source": [
    "###  Evaluation du réseau entraîné\n",
    " Lors de l'entraînement, les moyennes de la loss sur le jeu d'entrâinement et de la précision obtenue sur le jeu de validation sont enregistrées tous les 5 batchs dans :\n",
    "liste_loss liste_acc_val\n",
    "On peut donc les représenter graphiquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('batchs')\n",
    "ax1.set_ylabel('valiadation accuracy', color=color)\n",
    "ax1.plot(liste_acc_val, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('entropy loss', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(liste_loss, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3124f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = iter(valid_loader)\n",
    "dico = next(t)\n",
    "\n",
    "images =dico[\"image\"]\n",
    "labels =dico[\"label\"]\n",
    "tabular =dico[\"tabular\"]\n",
    "\n",
    "pred = np.array(torch.argmax(net(images.to(device),tabular.to(device)),dim =1).cpu())\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(pred) \n",
    "print(labels)\n",
    "\n",
    "confusion_matrix(pred,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96327793-de9d-4b89-bafd-f025d8efea19",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8fb870-1255-42da-9363-4d1a14a32f17",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "To start, let's do some feature engineering! We'll first take a look at the unique values for each variable in the metadatatrain_metadata.age.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc1c82-98e0-4537-ae99-2ee1ca884f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_metadata.sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378fdb86-e820-4948-81fb-4f0a8790b7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_metadata.body_site.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce91f3a-6ff4-4ebe-a542-fdb2ad6d9f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_metadata.melanoma_history.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4710dfc5-4c20-417b-8311-a52912da4b7a",
   "metadata": {},
   "source": [
    "So far, we've learned:\n",
    "\n",
    "- age is available to us in two-year intervals\n",
    "- melanoma_history is sometimes nan\n",
    "- body_site categories are not all at the same level of aggregation - for example, some slides are labeled as being from \"hand/foot/nail\" while some slides are labeled as from the hand, foot, and nail individually.\n",
    "\n",
    "Let's do the following:\n",
    "\n",
    "- convert age to integer values\n",
    "- convert melanoma_history to dummy variables\n",
    "- For this first pass, we will not use body_site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3688449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an encoder for melanoma_history, body_site \n",
    "enc = OneHotEncoder(drop=\"first\", sparse_output=False)\n",
    "df_enc = train_metadata[['melanoma_history','body_site']].copy()\n",
    "df_enc['body_site']=train_metadata['body_site'].str.replace(' ','_').str.replace('/','_')\n",
    "enc.fit(df_enc)\n",
    "enc.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4c9e1-293f-4c1c-ad03-a81e8595b45e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_feats(df=train_metadata):\n",
    "    feats = df.copy()\n",
    "    # take the first age in the range and convert to integer\n",
    "    feats[\"age_int\"] = feats.age.str.slice(1, 3).astype(int)\n",
    "    feats['body_site']=feats['body_site'].str.replace(' ','_').str.replace('/','_')\n",
    "    X = pd.concat(\n",
    "        [\n",
    "            feats[[\"filename\",\"age_int\", \"sex\",\"resolution\"]],\n",
    "            pd.DataFrame(\n",
    "                enc.transform(feats[[\"melanoma_history\",\"body_site\"]]),\n",
    "                columns=enc.get_feature_names_out(),\n",
    "                index=feats.index,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba04c6a-9967-4c34-9f21-01b3b211d876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocess features\n",
    "\n",
    "X = preprocess_feats(train_metadata)\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408f266-f41f-460b-a145-61e1c80d6417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take labels from train_labels.csv\n",
    "\n",
    "y = train_labels.relapse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c309ee4-3acc-480d-9e7e-98a6ea453f99",
   "metadata": {},
   "source": [
    "## Model training and calibration\n",
    "For a first pass at training, we will have a simple stratified 80-20 split between train and test. The stratified split helps ensure that the proportion of relapse cases are similar between our train and test set and that we are performing our model training on a representative subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b6cc2-059a-4c1b-8805-7720ef59afef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a2cdc-1ba3-4fb9-842a-c0e06c07e7fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc46f6b3-fb18-4a35-905d-1fd93131faad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25827b95-5413-48ee-9e30-0ddbe9c3bdf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_best_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3379b-1244-4071-99d6-21fac713e3ae",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e55109",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "params = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "search = RandomizedSearchCV(rf, param_distributions=params, random_state=42, n_iter=200, cv=5, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "search.fit(X_train,y_train)\n",
    "\n",
    "report_best_scores(search.cv_results_,n_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a1d9d-4825-4ce1-8f10-a539e9c102ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit a calibrated random forest model\n",
    "\n",
    "rf = RandomForestClassifier(random_state=SEED,n_estimators = 200,min_samples_split = 2,min_samples_leaf = 4,\n",
    "max_features = 'sqrt', max_depth = 2, bootstrap = True)\n",
    "calibrated_rf = CalibratedClassifierCV(rf, method=\"sigmoid\", cv=5)\n",
    "calibrated_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3af29b-d3eb-4313-a3a9-a54a224bc90d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353e149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGB = GradientBoostingClassifier()\n",
    "\n",
    "params = {\"learning_rate\": uniform(),\n",
    "            \"subsample\"    : uniform(),\n",
    "                  \"n_estimators\" : randint(100, 500),\n",
    "                  \"max_depth\"    : randint(2, 10)\n",
    "                 }\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=5, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "search.fit(X_train,y_train)\n",
    "\n",
    "report_best_scores(search.cv_results_,n_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033ea30-9a55-4009-9a86-0762a98d1b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelGB = GradientBoostingClassifier(learning_rate = 0.01083765148029836,n_estimators=175,\n",
    "max_depth = 2, subsample = 0.3549051904627206)\n",
    "calibrated_GB = CalibratedClassifierCV(modelGB, method=\"sigmoid\", cv=5)\n",
    "modelGB_fit = calibrated_GB.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "582006ba",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd990817",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": randint(2, 10), # default 3\n",
    "    \"n_estimators\": randint(100, 200), # default 100\n",
    "    \"learning_rate\": uniform(0.01, 0.3), # default 0.1 \n",
    "    \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "    \"subsample\": uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=5, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "search.fit(X_train,y_train)\n",
    "\n",
    "report_best_scores(search.cv_results_,n_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "random_state=SEED,\n",
    "max_depth = 2,\n",
    "n_estimators = 193,\n",
    "learning_rate = 0.023800792606525824,\n",
    "colsample_bytree=0.7580363202534581,\n",
    "subsample=0.9421842336044028)\n",
    "calibrated_xgb = CalibratedClassifierCV(xgb_model, method=\"sigmoid\", cv=5)\n",
    "xbg_model_fit = calibrated_xgb.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a8b1793",
   "metadata": {},
   "source": [
    "### Neural Network (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc273171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input functions\n",
    "\n",
    "batch_size = 100\n",
    "train_steps = 1000\n",
    "\n",
    "\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    return dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features=dict(features)\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17fb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Tensorflow feature columns\n",
    "feature_columns = [tf.feature_column.numeric_column(x) for x in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an Estimator\n",
    "\n",
    "# Build a DNN with 2 hidden layers and 10 nodes in each hidden layer.\n",
    "model = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                    hidden_units=[1024, 512, 256],\n",
    "                                    optimizer=tf.keras.optimizers.legacy.Adam(lr=0.0001))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75eeef20",
   "metadata": {},
   "source": [
    "### Train, Evaluate, and Predict\n",
    "Now that we have an Estimator object, we can call methods to do the following:\n",
    "\n",
    "Train the model.\n",
    "Evaluate the trained model.\n",
    "Use the trained model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc68f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "#The steps argument tells the method to stop training after a number of training steps.\n",
    "\n",
    "model.train(input_fn=lambda:train_input_fn(X_train, y_train, batch_size), steps=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the trained model\n",
    "#Now that the model has been trained, we can get some statistics on its performance. The following code block evaluates the accuracy of the trained model on the test data.\n",
    "# Evaluate the model.\n",
    "eval_result = model.evaluate(\n",
    "    input_fn=lambda:eval_input_fn(X_test, y_test, batch_size))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.2f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f64b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9dc47-c668-4506-9b82-34887acdbeac",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78902390-08ec-433e-b0ba-8e7f19009f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lr = LogisticRegression()\n",
    "# stack = StackingClassifier(classifiers=[calibrated_rf, calibrated_GB], meta_classifier=lr)\n",
    "# stack.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4a357-d710-42ff-856b-8c57f8598bfc",
   "metadata": {},
   "source": [
    "## Predicting on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5465ed3-8d86-4543-90ee-f1474f7aa285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score(y_true, y_pred):\n",
    "    return log_loss(y_true, y_pred, eps=1e-16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94b045-be34-421f-a5b6-847f064e14d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = calibrated_rf.predict_proba(X_test)[:, 1]\n",
    "score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b03ad-8795-47c8-97e3-b965beac11eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_GB = modelGB_fit.predict_proba(X_test)[:, 1]\n",
    "score(y_test, preds_GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_xgb = xbg_model_fit.predict_proba(X_test)[:, 1]\n",
    "score(y_test, preds_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b0d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(model.predict(\n",
    "    input_fn=lambda:eval_input_fn(X_test, y_test, batch_size)))\n",
    "\n",
    "# Dictionary for predictions\n",
    "col1 = []\n",
    "col2 = []\n",
    "col3 = []\n",
    "\n",
    "\n",
    "for idx, input, p in zip(X_test.index, y_test, predictions):\n",
    "    v  = p[\"class_ids\"][0] \n",
    "    class_id = p['class_ids'][0]\n",
    "    probability = p['probabilities'][1] # Probability\n",
    "    \n",
    "    # Adding to dataframe\n",
    "    col1.append(idx) # Index\n",
    "    col2.append(probability) # Prediction\n",
    "    col3.append(input) # Expecter\n",
    "    \n",
    "   \n",
    "    #print(template.format(idx, v, 100 * probability, input))\n",
    "\n",
    "\n",
    "results = pd.DataFrame({'index':col1, 'prediction_proba':col2, 'expected':col3})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e506ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score(results.expected,results.prediction_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f8011-ee16-41c2-982c-b481cacfb66e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preds_stack = stack.predict_proba(X_test)[:, 1]\n",
    "# score(y_test, preds_stack)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ede8d155",
   "metadata": {},
   "source": [
    "### TEST SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e23544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_feats(df, enc):\n",
    "    feats = df.copy()\n",
    "    # take the first age in the range and convert to integer\n",
    "    feats[\"age_int\"] = feats.age.str.slice(1, 3).astype(int)\n",
    "    feats['body_site']=feats['body_site'].str.replace(' ','_').str.replace('/','_')\n",
    "    X = pd.concat(\n",
    "        [\n",
    "            feats[[\"age_int\", \"sex\",\"resolution\"]],\n",
    "            pd.DataFrame(\n",
    "                enc.transform(feats[[\"melanoma_history\",\"body_site\"]]),\n",
    "                columns=enc.get_feature_names_out(),\n",
    "                index=feats.index,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return X\n",
    "\n",
    "processed_features = preprocess_feats(test_metadata, enc)\n",
    "\n",
    "predsub = calibrated_xgb.predict_proba(processed_features)[:,1]\n",
    "\n",
    "score(test_labels.relapse,predsub)\n",
    "\n",
    "# predictions = list(model.predict(\n",
    "#     input_fn=lambda:eval_input_fn(processed_features, test_labels.relapse, batch_size)))\n",
    "\n",
    "# # Dictionary for predictions\n",
    "# col1 = []\n",
    "# col2 = []\n",
    "# col3 = []\n",
    "\n",
    "\n",
    "# for idx, input, p in zip(test_metadata.index, test_labels, predictions):\n",
    "#     v  = p[\"class_ids\"][0] \n",
    "#     class_id = p['class_ids'][0]\n",
    "#     probability = p['probabilities'][1] # Probability\n",
    "    \n",
    "#     # Adding to dataframe\n",
    "#     col1.append(idx) # Index\n",
    "#     col2.append(probability) # Prediction\n",
    "#     col3.append(input) # Expecter\n",
    "    \n",
    "   \n",
    "#     #print(template.format(idx, v, 100 * probability, input))\n",
    "\n",
    "\n",
    "# results = pd.DataFrame({'index':col1, 'prediction_proba':col2, 'expected':col3})\n",
    "# results.head()\n",
    "\n",
    "# score(results.expected,results.prediction_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f4364b-7502-4479-bfc9-87661c3dfa65",
   "metadata": {},
   "source": [
    "# CODE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ea4aa-f01a-45e4-84e2-768964f24445",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"submission/assets\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# save out model to assets directory in submission folder\n",
    "dump(calibrated_rf, \"submission/assets/random_forest_model.joblib\")\n",
    "dump(calibrated_GB, \"submission/assets/gradient_boosting_model.joblib\")\n",
    "dump(xbg_model_fit, \"submission/assets/xgboost_model.joblib\")\n",
    "\n",
    "# save out encoder to assets directory as well\n",
    "dump(enc, \"submission/assets/hotencoder.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0d952-016b-456d-97f1-fe973634618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "\n",
    "\"\"\"Solution for VisioMel Challenge\"\"\"\n",
    "\n",
    "from joblib import load\n",
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DATA_ROOT = Path(\"/code_execution/data/\")\n",
    "\n",
    "\n",
    "def preprocess_feats(df, enc):\n",
    "    feats = df.copy()\n",
    "    # take the first age in the range and convert to integer\n",
    "    feats[\"age_int\"] = feats.age.str.slice(1, 3).astype(int)\n",
    "    feats['body_site']=feats['body_site'].str.replace(' ','_').str.replace('/','_')\n",
    "    X = pd.concat(\n",
    "        [\n",
    "            feats[[\"age_int\", \"sex\",\"resolution\"]],\n",
    "            pd.DataFrame(\n",
    "                enc.transform(feats[[\"melanoma_history\",\"body_site\"]]),\n",
    "                columns=enc.get_feature_names_out(),\n",
    "                index=feats.index,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return X\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    # load sumission format\n",
    "    submission_format = pd.read_csv(DATA_ROOT / \"submission_format.csv\", index_col=0)\n",
    "    \n",
    "    # load test_metadata\n",
    "    test_metadata = pd.read_csv(DATA_ROOT / \"test_metadata.csv\", index_col=0)\n",
    "    \n",
    "    logger.info(\"Loading feature encoder and model\")\n",
    "    calibrated_xgb = load(\"assets/xgboost_model.joblib\")\n",
    "    enc = load(\"assets/hotencoder.joblib\")\n",
    "    \n",
    "    logger.info(\"Preprocessing features\")\n",
    "    processed_features = preprocess_feats(test_metadata, enc)\n",
    "        \n",
    "    logger.info(\"Checking test feature filenames are in the same order as the submission format\")\n",
    "    assert (processed_features.index == submission_format.index).all()\n",
    "    \n",
    "    logger.info(\"Checking test feature columns align with loaded model\")\n",
    "    assert (processed_features.columns == calibrated_xgb.feature_names_in_).all()\n",
    "    \n",
    "    logger.info(\"Generating predictions\")\n",
    "    submission_format[\"relapse\"] = calibrated_xgb.predict_proba(processed_features)[:,1]\n",
    "\n",
    "    # save as \"submission.csv\" in the root folder, where it is expected\n",
    "    logger.info(\"Writing out submission.csv\")\n",
    "    submission_format.to_csv(\"submission.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe005f-2879-4a67-92ba-f2575d47a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip up submission\n",
    "! cd submission; zip -r ../submission.zip ./*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
