{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86cab7bc-d128-4759-8857-33d2b480b620",
   "metadata": {},
   "source": [
    "# Library imports, configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0e13c-f8d9-44cc-ad54-b29afc2ca97c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import cv2\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, randint\n",
    "from PIL import Image\n",
    "import pyvips\n",
    "import torch\n",
    "from joblib import dump\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from cloudpathlib import S3Path, S3Client, CloudPath\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score, mean_squared_error, precision_recall_curve, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import binarize, LabelEncoder, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "# from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout\n",
    "# from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# import tensorflow_decision_forests as tfdf\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e586f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() : device= torch.device(\"cuda:0\" ) \n",
    "else : device  = \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "if device != \"cpu\" :\n",
    "    print(\"nom du GPU :\", torch.cuda.get_device_name(device=None))\n",
    "    print(\"GPU initialis√© : \", torch.cuda.is_initialized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf2d5d-84cb-4d73-aea8-495415f508fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1965e11-6da1-4989-af77-332ac61692f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e929bc-b32d-409a-a1f9-0d71c235f446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET = \"jplaton/diffusion\"\n",
    "fs.ls(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d8435-f3fb-44f2-9da5-a92db084e066",
   "metadata": {},
   "source": [
    "# DATA Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa4da4-e041-47d9-9498-ae9877d00565",
   "metadata": {},
   "source": [
    "## Metadata and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece4bd5-0ed8-4fb0-bc1b-7cd8b0ff2c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATH_TRAIN = \"visio_mel/train_labels.csv\"\n",
    "FILE_PATH_TRAIN_S3 = BUCKET + \"/\" + FILE_PATH_TRAIN\n",
    "\n",
    "with fs.open(FILE_PATH_TRAIN_S3, mode=\"rb\") as file_in:\n",
    "    train_labels = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a665d2-1646-40e5-ac69-2e7369877ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATH_TRAIN = \"visio_mel/train_metadata.csv\"\n",
    "FILE_PATH_TRAIN_S3 = BUCKET + \"/\" + FILE_PATH_TRAIN\n",
    "\n",
    "with fs.open(FILE_PATH_TRAIN_S3, mode=\"rb\") as file_in:\n",
    "    train_metadata = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef29cb-1379-4f8f-8bb1-2423b5650780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATH_TEST = \"visio_mel/visiomel_code_execution_development_data/test_labels.csv\"\n",
    "FILE_PATH_TEST_S3 = BUCKET + \"/\" + FILE_PATH_TEST\n",
    "\n",
    "with fs.open(FILE_PATH_TEST_S3, mode=\"rb\") as file_in:\n",
    "    test_labels = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1975147-9b12-4a6e-92f9-84e82f4737a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATH_TEST = \"visio_mel/visiomel_code_execution_development_data/test_metadata.csv\"\n",
    "FILE_PATH_TEST_S3 = BUCKET + \"/\" + FILE_PATH_TEST\n",
    "\n",
    "with fs.open(FILE_PATH_TEST_S3, mode=\"rb\") as file_in:\n",
    "    test_metadata = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af98f87-a877-420e-b461-a2607032e930",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c3eca-88de-4f2f-88a3-91bb2baa3b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select a set of images to plot\n",
    "SEED = 42\n",
    "NUM_IMAGES = 3\n",
    "\n",
    "# we'll use the US url\n",
    "selected_image_paths = train_metadata.sample(\n",
    "    random_state=SEED, n=NUM_IMAGES\n",
    ").us_tif_url.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209e935-8680-4ddb-a5c2-7882b3158214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_page(page_num, image_paths):\n",
    "    fig, axes = plt.subplots(1, len(image_paths), figsize=(10, 10))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for n, image_s3_path in enumerate(image_paths):\n",
    "        fname = S3Path(image_s3_path).name\n",
    "        print(f\"Downloading {fname}\")\n",
    "        local_file = S3Path(image_s3_path,client = S3Client(no_sign_request=True)).fspath #local_cache_dir='/tmp/images', \n",
    "\n",
    "        n_frames = Image.open(local_file).n_frames\n",
    "        img = pyvips.Image.new_from_file(local_file, page=page_num).numpy()\n",
    "\n",
    "        axes[n].set_title(f\"{fname}, page={page_num}\\n {img.shape}\\n\")\n",
    "        axes[n].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b5570-accb-4ac8-8da6-c6f81c7a04cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's look at a few high-resolution pages from our selected images\n",
    "visualize_page(3, selected_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25074666-fd8a-4f30-a36b-4b9086f3c023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's look at low-resolution pages from the same images\n",
    "visualize_page(8, selected_image_paths)\n",
    "img = pyvips.Image.new_from_file('../data/images/9k10x5h3.tif', page=0).numpy()\n",
    "#plt.imshow(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f91ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray = rgb2gray(img)\n",
    "img_tresh = img_gray < 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c86795",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_tresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff76e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray[img_tresh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bis = pyvips.Image.new_from_file('../data/images/9k10x5h3.tif', page=8)\n",
    "#print(img_bis.width, img_bis.height)\n",
    "img_bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7964c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_bis)\n",
    "plt.plot(53696, 103232, \"og\", markersize=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f152c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0546122",
   "metadata": {},
   "source": [
    "### Cr√©ation du dataset d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_file = os.listdir(\"../data/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff6703",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_im = train_labels.copy()\n",
    "train_labels_im = train_labels_im[train_labels_im['filename'].isin(train_images_file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31761806",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_im = train_metadata.copy()\n",
    "train_metadata_im = train_metadata_im[train_metadata_im['filename'].isin(train_images_file)]\n",
    "train_metadata_im['filepaths'] = '../data/images/'+ train_metadata_im['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_paths = np.array(train_metadata_im['filepaths'])\n",
    "train_images_labels = np.array(train_labels_im['relapse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a441d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images_name = np.array(train_metadata['filename'])\n",
    "# train_images_labels_all = np.array(train_labels['relapse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e00f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_df = X.copy()\n",
    "tabular_df = tabular_df[tabular_df['filename'].isin(train_images_file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation de la classe permettant d'acc√©der au dataset\n",
    "# On cr√©e ici la classe Custom Dataset h√©ritant de la classe Dataset en python\n",
    "# Un CustomDataset sera initialis√© avec l'ensemble des images_paths et ses labels pr√©cedemment pr√©sent√©s.\n",
    "# Un objet Dataset sera dit it√©rable dans la mesure o√π on pourra boucler dessus pour en r√©cup√©rer les √©l√©ments. A chaque it√©ration d'une telle boucle La fonction getitem  sera appel√©e pour acc√©der √† un √©l√©ment. \n",
    "# Ci-dessous on demande √† chaque it√©ration de retourner un dictionnnaire dont les items (clef,valeur)  sont : \n",
    "# ('image', l'image sous forme d'array ) ('label', le label associ√© (0,1))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths,labels,tabular_df): \n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.tabular = tabular_df\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        tabular = self.tabular.iloc[idx, 0:]\n",
    "        \n",
    "        #local_file = S3Path('s3://drivendata-competition-visiomel-public-us/images/'+self.image_paths[idx],client=S3Client(local_cache_dir='/tmp/images',no_sign_request=True)).fspath \n",
    "        #image = pyvips.Image.new_from_file(local_file, page=3).numpy()\n",
    "        image = pyvips.Image.new_from_file(self.image_paths[idx], page=3).numpy()  \n",
    "        image = cv2.resize(image, (512, 512))\n",
    "        image = torch.tensor(np.array(image,dtype = float)/255, dtype =torch.float).permute(2,1,0)     \n",
    "        label = self.labels[idx]\n",
    "        #os.remove(local_file)\n",
    "\n",
    "        tabular = tabular[['age_int','sex','resolution','melanoma_history_YES', 'melanoma_history_nan', 'body_site_face',\n",
    "       'body_site_finger', 'body_site_foot', 'body_site_forearm',\n",
    "       'body_site_hand', 'body_site_hand_foot_nail',\n",
    "       'body_site_head_neck', 'body_site_leg', 'body_site_lower_limb_hip',\n",
    "       'body_site_nail', 'body_site_neck', 'body_site_scalp',\n",
    "       'body_site_seat', 'body_site_sole', 'body_site_thigh',\n",
    "       'body_site_toe', 'body_site_trunc', 'body_site_trunk',\n",
    "       'body_site_upper_limb_shoulder', 'body_site_nan']]\n",
    "\n",
    "        tabular = tabular.tolist()\n",
    "        tabular = torch.FloatTensor(tabular)\n",
    "        \n",
    "     \n",
    "        return {\"image\": image, \"tabular\": tabular, \"label\" : label} \n",
    "        \n",
    "        \n",
    "    def __len__(self):  # return count of sample we have\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57b3426a",
   "metadata": {},
   "source": [
    "###  D√©finition de quelques hyper param√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c086ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASSES = 2\n",
    "config ={\n",
    "    'n_epoch' : 100,\n",
    "    'val_size' : 0.20,\n",
    "    'batch_size' : 4,\n",
    "    'optimizer' : \"SGD\",\n",
    "    'lr' : 0.005,\n",
    "    'momentum' : 0.9,\n",
    "    'model type': \"convnet\",\n",
    "    'descriptif': \"Entrainement avec un convnet\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3792d8a7",
   "metadata": {},
   "source": [
    "### Cr√©ation des Data Loader\n",
    "Dans un premier temps on va d√©couper notre jeu d'apprentissage (les 366 images labellis√©es) en : \n",
    "un jeu de d'entra√Ænement sur lequel on  entra√Ænera notre mod√®le un jeu de validation ne participant pas √† l'entrainement du mod√®le mais permettant d'en √©valuer les performances\n",
    "La fonction train_test_split de scikit learn permet de faire ce d√©coupage. Le param√®tre val_size de la config donne la proportion ddu jeu d'images total (entre 0 et 1) que l'on veut garder dans le jeu de validation. Le param√®tre stratify permet de pr√©ciser sur quel crit√®re on veut stratifier cette s√©lection al√©atoire (ici les labels dans le but d'avoir un √©chantillon repr√©sentatif des labels de l'ensemble des images)Lors de l'entra√Ænement, on va en fait entrainer notre algorithme par it√©ration sur des petits paquets d'images (appel√©s batchs). On calcule pour chacun de ces batchs l'erreur commise par l'algorithme et on modifie ses param√®tres en cons√©quence (par descente de gradient)Cette extraction par paquet d'images est permise par le Dataloader moyenant le remplissage du param√®tre batch_size (cf 'batch_size' dans la config) permettant de pr√©ciser la taille des batchs que l'on souhaite avoir. Le param√®tre shuffle √©gal √† True veut dire que les images seront toutes rem√©lang√©es une fois un tour total de l'ensemble des images r√©alis√©. \n",
    "On appelle un tour complet une epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1bfd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  config['batch_size']\n",
    "all_dataset = CustomDataset(train_images_paths,train_images_labels,tabular_df)\n",
    "#all_dataset = CustomDataset(train_images_name,train_images_labels_all)\n",
    "\n",
    "# je d√©coupe le data set en train et validation (20 % de validation) en stratifiant par les labels\n",
    "train_indices, valid_indices = train_test_split(list(range(len(train_images_labels))), test_size=config['val_size'], stratify=train_images_labels)\n",
    "#train_indices, valid_indices = train_test_split(list(range(len(train_images_labels_all))), test_size=config['val_size'], stratify=train_images_labels_all)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(all_dataset, train_indices)\n",
    "valid_dataset = torch.utils.data.Subset(all_dataset, valid_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f4d88be",
   "metadata": {},
   "source": [
    "### D√©finition des mod√®les\n",
    " On d√©finit ici une classe Net (h√©ritant de la classe module) permettant d'instancier des r√©seaux de neurones convolutifs.Dans init, on d√©finit l'ensemble des objets utilis√©s dans l'op√©ration forward.L'op√©ration forward d√©finit la suite des op√©rations qui seront appliqu√©es aux images auxquelles on appliquera un mod√®le de type Net.Dans la fonction forward (qui prend en entr√©e une image x) on constate bien que plusieurs transformations enboit√©es sont successivement appliqu√©es √† l'image x en entr√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_size, output_size):\n",
    "    block = nn.Sequential(\n",
    "        nn.Conv2d(input_size, output_size, (3, 3)), nn.ReLU(), nn.BatchNorm2d(output_size), nn.MaxPool2d((2, 2)),\n",
    "    )\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96531d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_block(3, 16)\n",
    "        self.conv2 = conv_block(16, 32)\n",
    "        self.conv3 = conv_block(32, 64)\n",
    "\n",
    "        self.ln1 = nn.Linear(246016, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(16)\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.ln2 = nn.Linear(16, 5)\n",
    "\n",
    "        self.ln4 = nn.Linear(25, 10)\n",
    "        self.ln5 = nn.Linear(10, 10)\n",
    "        self.ln6 = nn.Linear(10, 5)\n",
    "        self.ln7 = nn.Linear(10, NB_CLASSES)\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # self.conv3 = nn.Conv2d(16, 32, 4)\n",
    "        # self.conv4 = nn.Conv2d(32, 16, 3)\n",
    "        # self.conv5 = nn.Conv2d(16, 24, 3)\n",
    "        # self.fc1 = nn.Linear(256, 128)\n",
    "        # self.fc2 = nn.Linear(128, 64)\n",
    "        # self.fc3 = nn.Linear(64, NB_CLASSES)\n",
    "        \n",
    "    def forward(self, img, tab):\n",
    "        \n",
    "        img = self.conv1(img)\n",
    "        img = self.conv2(img)\n",
    "        img = self.conv3(img)\n",
    "        img = img.reshape(img.shape[0], -1)\n",
    "        img = self.ln1(img)\n",
    "        img = self.relu(img)\n",
    "        img = self.batchnorm(img)\n",
    "        img = self.dropout(img)\n",
    "        img = self.ln2(img)\n",
    "        img = self.relu(img)\n",
    "\n",
    "        tab = self.ln4(tab)\n",
    "        tab = self.relu(tab)\n",
    "        tab = self.ln5(tab)\n",
    "        tab = self.relu(tab)\n",
    "        tab = self.ln6(tab)\n",
    "        tab = self.relu(tab)\n",
    "\n",
    "        x = torch.cat((img, tab), dim=1)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return self.ln7(x)\n",
    "        # x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x = self.pool(F.relu(self.conv3(x)))\n",
    "        # x = self.pool(F.relu(self.conv4(x)))\n",
    "        # x = self.pool(F.relu(self.conv5(x)))\n",
    "        # x = torch.flatten(x, 1)# flatten all dimensions except batch\n",
    "        # #print(x.size())\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        # return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf5a0cb9",
   "metadata": {},
   "source": [
    "###  Test Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822a924",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "get_n_params(net) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59578b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=config['lr'])\n",
    "net = net.to(device)\n",
    "\n",
    "entropy = nn.CrossEntropyLoss() \n",
    "\n",
    "liste_loss = []\n",
    "liste_acc_val = []\n",
    "for epoch in range(config['n_epoch']):  \n",
    "    \n",
    "    net = net.to(device)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    t= tqdm(train_loader, desc=\"epoch %i\" % (epoch+1),position = 0, leave=True)\n",
    "    epoch_loop = enumerate(t)\n",
    "\n",
    "    for i, data in epoch_loop:\n",
    "        \n",
    "        taille_batch = data['image'].shape[0]\n",
    "        images = data['image']\n",
    "        labels  =  data['label']\n",
    "        tabular = data['tabular']\n",
    "            \n",
    "        images, labels, tabular = images.to(device), labels.to(device), tabular.to(device)\n",
    "\n",
    "        pred = net(images,tabular)\n",
    "            \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = entropy(pred,labels)\n",
    "            \n",
    "        loss.backward() # calculer le gradient\n",
    "        optimizer.step() # avancer dans le sens du gradient calcul√©\n",
    "\n",
    "        del images, labels, pred\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (i+1) % 10 == 0:  \n",
    "            # ici enregistrement de la loss sur le train, sur le validation et envoi des r√©sultats √† wnandb\n",
    "            liste_loss.append(running_loss)\n",
    "\n",
    "            # validation\n",
    "            ech_val = iter(valid_loader)\n",
    "            dico = next(ech_val)\n",
    "            images =dico[\"image\"]\n",
    "            labels =dico[\"label\"]\n",
    "            tabular =dico[\"tabular\"]\n",
    "\n",
    "            pred = torch.argmax(net(images.to(device),tabular.to(device)),dim =1)\n",
    "            labels = labels.to(device)\n",
    "                \n",
    "            acc_val =round(100*int(torch.sum(pred == labels).cpu())/int(np.array(labels.size())),2)\n",
    "            liste_acc_val.append(acc_val)\n",
    "            t.set_description(\"epoch %i, 'mean loss: %.6f','acc.val: %.2f'\" % (epoch+1,running_loss/10,acc_val))\n",
    "            t.refresh()\n",
    "                \n",
    "            running_loss =0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbacd196",
   "metadata": {},
   "source": [
    "###  Evaluation du r√©seau entra√Æn√©\n",
    " Lors de l'entra√Ænement, les moyennes de la loss sur le jeu d'entr√¢inement et de la pr√©cision obtenue sur le jeu de validation sont enregistr√©es tous les 5 batchs dans :\n",
    "liste_loss liste_acc_val\n",
    "On peut donc les repr√©senter graphiquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('batchs')\n",
    "ax1.set_ylabel('valiadation accuracy', color=color)\n",
    "ax1.plot(liste_acc_val, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('entropy loss', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(liste_loss, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3124f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = iter(valid_loader)\n",
    "dico = next(t)\n",
    "\n",
    "images =dico[\"image\"]\n",
    "labels =dico[\"label\"]\n",
    "tabular =dico[\"tabular\"]\n",
    "\n",
    "pred = np.array(torch.argmax(net(images.to(device),tabular.to(device)),dim =1).cpu())\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(pred) \n",
    "print(labels)\n",
    "\n",
    "confusion_matrix(pred,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96327793-de9d-4b89-bafd-f025d8efea19",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8fb870-1255-42da-9363-4d1a14a32f17",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "To start, let's do some feature engineering! We'll first take a look at the unique values for each variable in the metadatatrain_metadata.age.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc1c82-98e0-4537-ae99-2ee1ca884f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_metadata.sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378fdb86-e820-4948-81fb-4f0a8790b7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_metadata.body_site.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce91f3a-6ff4-4ebe-a542-fdb2ad6d9f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_metadata.melanoma_history.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4710dfc5-4c20-417b-8311-a52912da4b7a",
   "metadata": {},
   "source": [
    "So far, we've learned:\n",
    "\n",
    "- age is available to us in two-year intervals\n",
    "- melanoma_history is sometimes nan\n",
    "- body_site categories are not all at the same level of aggregation - for example, some slides are labeled as being from \"hand/foot/nail\" while some slides are labeled as from the hand, foot, and nail individually.\n",
    "\n",
    "Let's do the following:\n",
    "\n",
    "- convert age to integer values\n",
    "- convert melanoma_history to dummy variables\n",
    "- For this first pass, we will not use body_site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3688449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an encoder for melanoma_history, body_site \n",
    "enc = OneHotEncoder(drop=\"first\", sparse_output=False)\n",
    "df_enc = train_metadata[['melanoma_history','body_site']].copy()\n",
    "df_enc['body_site']=train_metadata['body_site'].str.replace(' ','_').str.replace('/','_')\n",
    "enc.fit(df_enc)\n",
    "enc.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4c9e1-293f-4c1c-ad03-a81e8595b45e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_feats(df=train_metadata):\n",
    "    feats = df.copy()\n",
    "    # take the first age in the range and convert to integer\n",
    "    feats[\"age_int\"] = feats.age.str.slice(1, 3).astype(int)\n",
    "    feats['body_site']=feats['body_site'].str.replace(' ','_').str.replace('/','_')\n",
    "    X = pd.concat(\n",
    "        [\n",
    "            feats[[\"filename\",\"age_int\", \"sex\",\"resolution\"]],\n",
    "            pd.DataFrame(\n",
    "                enc.transform(feats[[\"melanoma_history\",\"body_site\"]]),\n",
    "                columns=enc.get_feature_names_out(),\n",
    "                index=feats.index,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba04c6a-9967-4c34-9f21-01b3b211d876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocess features\n",
    "\n",
    "X = preprocess_feats(train_metadata)\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408f266-f41f-460b-a145-61e1c80d6417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take labels from train_labels.csv\n",
    "\n",
    "y = train_labels.relapse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c309ee4-3acc-480d-9e7e-98a6ea453f99",
   "metadata": {},
   "source": [
    "## Model training and calibration\n",
    "For a first pass at training, we will have a simple stratified 80-20 split between train and test. The stratified split helps ensure that the proportion of relapse cases are similar between our train and test set and that we are performing our model training on a representative subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b6cc2-059a-4c1b-8805-7720ef59afef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a2cdc-1ba3-4fb9-842a-c0e06c07e7fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc46f6b3-fb18-4a35-905d-1fd93131faad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25827b95-5413-48ee-9e30-0ddbe9c3bdf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_best_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3379b-1244-4071-99d6-21fac713e3ae",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e55109",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "params = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "search = RandomizedSearchCV(rf, param_distributions=params, random_state=42, n_iter=200, cv=5, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "search.fit(X_train,y_train)\n",
    "\n",
    "report_best_scores(search.cv_results_,n_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a1d9d-4825-4ce1-8f10-a539e9c102ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit a calibrated random forest model\n",
    "\n",
    "rf = RandomForestClassifier(random_state=SEED,n_estimators = 200,min_samples_split = 2,min_samples_leaf = 4,\n",
    "max_features = 'sqrt', max_depth = 2, bootstrap = True)\n",
    "calibrated_rf = CalibratedClassifierCV(rf, method=\"sigmoid\", cv=5)\n",
    "calibrated_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3af29b-d3eb-4313-a3a9-a54a224bc90d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353e149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGB = GradientBoostingClassifier()\n",
    "\n",
    "params = {\"learning_rate\": uniform(),\n",
    "            \"subsample\"    : uniform(),\n",
    "                  \"n_estimators\" : randint(100, 500),\n",
    "                  \"max_depth\"    : randint(2, 10)\n",
    "                 }\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=5, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "search.fit(X_train,y_train)\n",
    "\n",
    "report_best_scores(search.cv_results_,n_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033ea30-9a55-4009-9a86-0762a98d1b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelGB = GradientBoostingClassifier(learning_rate = 0.01083765148029836,n_estimators=175,\n",
    "max_depth = 2, subsample = 0.3549051904627206)\n",
    "calibrated_GB = CalibratedClassifierCV(modelGB, method=\"sigmoid\", cv=5)\n",
    "modelGB_fit = calibrated_GB.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "582006ba",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd990817",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": randint(2, 10), # default 3\n",
    "    \"n_estimators\": randint(100, 200), # default 100\n",
    "    \"learning_rate\": uniform(0.01, 0.3), # default 0.1 \n",
    "    \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "    \"subsample\": uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=5, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "search.fit(X_train,y_train)\n",
    "\n",
    "report_best_scores(search.cv_results_,n_top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "random_state=SEED,\n",
    "max_depth = 2,\n",
    "n_estimators = 193,\n",
    "learning_rate = 0.023800792606525824,\n",
    "colsample_bytree=0.7580363202534581,\n",
    "subsample=0.9421842336044028)\n",
    "calibrated_xgb = CalibratedClassifierCV(xgb_model, method=\"sigmoid\", cv=5)\n",
    "xbg_model_fit = calibrated_xgb.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a8b1793",
   "metadata": {},
   "source": [
    "### Neural Network (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc273171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input functions\n",
    "\n",
    "batch_size = 100\n",
    "train_steps = 1000\n",
    "\n",
    "\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    return dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features=dict(features)\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17fb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Tensorflow feature columns\n",
    "feature_columns = [tf.feature_column.numeric_column(x) for x in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an Estimator\n",
    "\n",
    "# Build a DNN with 2 hidden layers and 10 nodes in each hidden layer.\n",
    "model = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                    hidden_units=[1024, 512, 256],\n",
    "                                    optimizer=tf.keras.optimizers.legacy.Adam(lr=0.0001))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75eeef20",
   "metadata": {},
   "source": [
    "### Train, Evaluate, and Predict\n",
    "Now that we have an Estimator object, we can call methods to do the following:\n",
    "\n",
    "Train the model.\n",
    "Evaluate the trained model.\n",
    "Use the trained model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc68f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "#The steps argument tells the method to stop training after a number of training steps.\n",
    "\n",
    "model.train(input_fn=lambda:train_input_fn(X_train, y_train, batch_size), steps=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the trained model\n",
    "#Now that the model has been trained, we can get some statistics on its performance. The following code block evaluates the accuracy of the trained model on the test data.\n",
    "# Evaluate the model.\n",
    "eval_result = model.evaluate(\n",
    "    input_fn=lambda:eval_input_fn(X_test, y_test, batch_size))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.2f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f64b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9dc47-c668-4506-9b82-34887acdbeac",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78902390-08ec-433e-b0ba-8e7f19009f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lr = LogisticRegression()\n",
    "# stack = StackingClassifier(classifiers=[calibrated_rf, calibrated_GB], meta_classifier=lr)\n",
    "# stack.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4a357-d710-42ff-856b-8c57f8598bfc",
   "metadata": {},
   "source": [
    "## Predicting on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5465ed3-8d86-4543-90ee-f1474f7aa285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score(y_true, y_pred):\n",
    "    return log_loss(y_true, y_pred, eps=1e-16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94b045-be34-421f-a5b6-847f064e14d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = calibrated_rf.predict_proba(X_test)[:, 1]\n",
    "score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b03ad-8795-47c8-97e3-b965beac11eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_GB = modelGB_fit.predict_proba(X_test)[:, 1]\n",
    "score(y_test, preds_GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_xgb = xbg_model_fit.predict_proba(X_test)[:, 1]\n",
    "score(y_test, preds_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b0d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(model.predict(\n",
    "    input_fn=lambda:eval_input_fn(X_test, y_test, batch_size)))\n",
    "\n",
    "# Dictionary for predictions\n",
    "col1 = []\n",
    "col2 = []\n",
    "col3 = []\n",
    "\n",
    "\n",
    "for idx, input, p in zip(X_test.index, y_test, predictions):\n",
    "    v  = p[\"class_ids\"][0] \n",
    "    class_id = p['class_ids'][0]\n",
    "    probability = p['probabilities'][1] # Probability\n",
    "    \n",
    "    # Adding to dataframe\n",
    "    col1.append(idx) # Index\n",
    "    col2.append(probability) # Prediction\n",
    "    col3.append(input) # Expecter\n",
    "    \n",
    "   \n",
    "    #print(template.format(idx, v, 100 * probability, input))\n",
    "\n",
    "\n",
    "results = pd.DataFrame({'index':col1, 'prediction_proba':col2, 'expected':col3})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e506ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score(results.expected,results.prediction_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f8011-ee16-41c2-982c-b481cacfb66e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preds_stack = stack.predict_proba(X_test)[:, 1]\n",
    "# score(y_test, preds_stack)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ede8d155",
   "metadata": {},
   "source": [
    "### TEST SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e23544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_feats(df, enc):\n",
    "    feats = df.copy()\n",
    "    # take the first age in the range and convert to integer\n",
    "    feats[\"age_int\"] = feats.age.str.slice(1, 3).astype(int)\n",
    "    feats['body_site']=feats['body_site'].str.replace(' ','_').str.replace('/','_')\n",
    "    X = pd.concat(\n",
    "        [\n",
    "            feats[[\"age_int\", \"sex\",\"resolution\"]],\n",
    "            pd.DataFrame(\n",
    "                enc.transform(feats[[\"melanoma_history\",\"body_site\"]]),\n",
    "                columns=enc.get_feature_names_out(),\n",
    "                index=feats.index,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return X\n",
    "\n",
    "processed_features = preprocess_feats(test_metadata, enc)\n",
    "\n",
    "predsub = calibrated_xgb.predict_proba(processed_features)[:,1]\n",
    "\n",
    "score(test_labels.relapse,predsub)\n",
    "\n",
    "# predictions = list(model.predict(\n",
    "#     input_fn=lambda:eval_input_fn(processed_features, test_labels.relapse, batch_size)))\n",
    "\n",
    "# # Dictionary for predictions\n",
    "# col1 = []\n",
    "# col2 = []\n",
    "# col3 = []\n",
    "\n",
    "\n",
    "# for idx, input, p in zip(test_metadata.index, test_labels, predictions):\n",
    "#     v  = p[\"class_ids\"][0] \n",
    "#     class_id = p['class_ids'][0]\n",
    "#     probability = p['probabilities'][1] # Probability\n",
    "    \n",
    "#     # Adding to dataframe\n",
    "#     col1.append(idx) # Index\n",
    "#     col2.append(probability) # Prediction\n",
    "#     col3.append(input) # Expecter\n",
    "    \n",
    "   \n",
    "#     #print(template.format(idx, v, 100 * probability, input))\n",
    "\n",
    "\n",
    "# results = pd.DataFrame({'index':col1, 'prediction_proba':col2, 'expected':col3})\n",
    "# results.head()\n",
    "\n",
    "# score(results.expected,results.prediction_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f4364b-7502-4479-bfc9-87661c3dfa65",
   "metadata": {},
   "source": [
    "# CODE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ea4aa-f01a-45e4-84e2-768964f24445",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"submission/assets\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# save out model to assets directory in submission folder\n",
    "dump(calibrated_rf, \"submission/assets/random_forest_model.joblib\")\n",
    "dump(calibrated_GB, \"submission/assets/gradient_boosting_model.joblib\")\n",
    "dump(xbg_model_fit, \"submission/assets/xgboost_model.joblib\")\n",
    "\n",
    "# save out encoder to assets directory as well\n",
    "dump(enc, \"submission/assets/hotencoder.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0d952-016b-456d-97f1-fe973634618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "\n",
    "\"\"\"Solution for VisioMel Challenge\"\"\"\n",
    "\n",
    "from joblib import load\n",
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DATA_ROOT = Path(\"/code_execution/data/\")\n",
    "\n",
    "\n",
    "def preprocess_feats(df, enc):\n",
    "    feats = df.copy()\n",
    "    # take the first age in the range and convert to integer\n",
    "    feats[\"age_int\"] = feats.age.str.slice(1, 3).astype(int)\n",
    "    feats['body_site']=feats['body_site'].str.replace(' ','_').str.replace('/','_')\n",
    "    X = pd.concat(\n",
    "        [\n",
    "            feats[[\"age_int\", \"sex\",\"resolution\"]],\n",
    "            pd.DataFrame(\n",
    "                enc.transform(feats[[\"melanoma_history\",\"body_site\"]]),\n",
    "                columns=enc.get_feature_names_out(),\n",
    "                index=feats.index,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return X\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    # load sumission format\n",
    "    submission_format = pd.read_csv(DATA_ROOT / \"submission_format.csv\", index_col=0)\n",
    "    \n",
    "    # load test_metadata\n",
    "    test_metadata = pd.read_csv(DATA_ROOT / \"test_metadata.csv\", index_col=0)\n",
    "    \n",
    "    logger.info(\"Loading feature encoder and model\")\n",
    "    calibrated_xgb = load(\"assets/xgboost_model.joblib\")\n",
    "    enc = load(\"assets/hotencoder.joblib\")\n",
    "    \n",
    "    logger.info(\"Preprocessing features\")\n",
    "    processed_features = preprocess_feats(test_metadata, enc)\n",
    "        \n",
    "    logger.info(\"Checking test feature filenames are in the same order as the submission format\")\n",
    "    assert (processed_features.index == submission_format.index).all()\n",
    "    \n",
    "    logger.info(\"Checking test feature columns align with loaded model\")\n",
    "    assert (processed_features.columns == calibrated_xgb.feature_names_in_).all()\n",
    "    \n",
    "    logger.info(\"Generating predictions\")\n",
    "    submission_format[\"relapse\"] = calibrated_xgb.predict_proba(processed_features)[:,1]\n",
    "\n",
    "    # save as \"submission.csv\" in the root folder, where it is expected\n",
    "    logger.info(\"Writing out submission.csv\")\n",
    "    submission_format.to_csv(\"submission.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe005f-2879-4a67-92ba-f2575d47a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip up submission\n",
    "! cd submission; zip -r ../submission.zip ./*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
